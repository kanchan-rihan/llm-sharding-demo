# LLM Sharding Concept

**A hands-on demo to learn the concept of sharding large language models (LLMs)**

This repository demonstrates how to split a Tiny GPT-2 model into multiple â€œshardsâ€ and orchestrate token generation using a **coordinator**. The goal is educational: to help you understand model sharding and distributed inference in a simple setup.

---

## ğŸ—ï¸ Overview

We split the Tiny GPT-2 model into two shards:

- **Shard A** â€“ the â€œprep chefâ€: handles embeddings and early transformer blocks.
- **Shard B** â€“ the â€œfinishing chefâ€: handles the remaining transformer blocks and produces final logits.
- **Coordinator** â€“ the â€œhead chefâ€: orchestrates generation by sending prompts to Shard A and hidden states to Shard B.

The architecture mimics a kitchen workflow:

```
User prompt
    â”‚
    â–¼
Coordinator (tokenizes prompt)
    â”‚
    â–¼
Shard A (partial forward pass)
    â”‚
    â–¼
Shard B (finishes forward pass â†’ logits)
    â”‚
    â–¼
Coordinator (selects next token & decodes text)
    â”‚
    â–¼
Generated text
```

---

## âš™ï¸ Requirements

- Python 3.10+
- PyTorch
- Transformers
- FastAPI
- Docker & Minikube (for local Kubernetes setup)
- Requests (for notebook client)

---

## ğŸš€ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/<your-username>/llm-sharding-demo.git
cd llm-sharding-demo
```

### 2. Build and run Docker images

This example uses **Minikube** for a local Kubernetes cluster:

```bash
# Install Minikube
brew install minikube

# Install Docker Desktop
brew install --cask docker

# Start Kubernetes cluster
minikube start --driver=docker --cpus=8 --memory=16g

# Enable metrics server (optional)
minikube addons enable metrics-server
```

Build the Docker image and deploy:

```bash
# Ensure commands run inside Minikube Docker env
eval $(minikube docker-env)

# Build image
docker build -t llm-sharding:latest .

# Deploy shards and coordinator to Kubernetes
kubectl apply -f k8s/
kubectl get pods -o wide
kubectl get svc
```

---

### 3. Run the coordinator

Before sending prompts, port-forward the coordinator service:

```bash
kubectl port-forward svc/llm-coordinator 5000:5000
```

---

### 4. Test generation using the notebook

The included Jupyter notebook demonstrates how to:

- Send a prompt to the coordinator
- Receive generated output
- Iterate token-by-token via Shard A and Shard B

Example usage:

```python
prompt = "Hi, "
output = generate_text(prompt, max_new_tokens=20)
print("Prompt:", prompt)
print("Generated output:", output)
```

---

## ğŸ“š Learning Goals

- Understand **model sharding** for LLMs.
- Learn how to orchestrate multiple shards via a **coordinator**.
- Explore **distributed inference workflows** locally with Kubernetes.

---

## ğŸ“ Notes

- This demo uses **CPU** for simplicity.
- Tiny GPT-2 is used to keep the example lightweight; results may not be coherent due to small model size.
- For more meaningful text, try larger models or avoid splitting very small models.
- The notebook demonstrates **single-shell execution** to avoid Docker/Minikube environment issues.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€ server.py         # FastAPI server for Shard A, Shard B, and Coordinator
â”œâ”€ requirements.txt
â”œâ”€ Dockerfile        # Docker build file for the service
â”œâ”€ notebook.ipynb    # Jupyter notebook to test generation
â”œâ”€ README.md         # This README
â””â”€ k8s/              # Kubernetes deployment YAMLs for coordinator and shards
   â”œâ”€ shard-a-deployment.yaml
   â”œâ”€ shard-b-deployment.yaml
   â”œâ”€ shard-a-service.yaml
   â”œâ”€ shard-b-service.yaml
   â”œâ”€ coordinator-deployment.yaml
   â””â”€ coordinator-service.yaml
```
